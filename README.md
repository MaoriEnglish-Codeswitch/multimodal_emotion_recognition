# multimodal_emotion_recognition
This code is for the paper: Improved Multi-modal Emotion Recognition using Squeeze-and-Exciation Block in Cross-Modal Attention

The field of multi-modal emotion recognition has gained significant attention due to its applications in human-computer interaction. This paper explores recent advancements and challenges in multi-modal emotion recognition. We propose a neural network-based multi-modal emotion recognition model that applies attention mechanisms to fuse features extracted by the transformer model and convolutional neural networks. Specifically, we use pre-trained models, including BERT, ResNet, and DenseNet, to extract features from text, spectrograms, and face images, combining convolutional and recurrent layers to capture temporal information. We integrate all extracted features using cross-modal attention. In addition, a novel squeeze-and-excitation that enhances the representation ability of individual features is proposed to find the fine distinction between emotions, which improves the model's recognition ability for similar emotional states. Our model is evaluated on the IEMOCAP and RAVDESS datasets,
